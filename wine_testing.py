# -*- coding: utf-8 -*-
"""Wine testing

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A01IXE5wtJahZCuM3TU1AluNHOkYTIFI
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

df=pd.read_csv('/content/wine_data.csv')

"""What is the most frequently occurring wine quality?
What is the highest number in and the lowest number in the quantity column?
"""

most_frequent_quality = df['quality'].mode()[0]
print(f"The most frequently occurring wine quality is: {most_frequent_quality}")
max_quality = df['quality'].max()
min_quality = df['quality'].min()
print(f"The highest value in the 'quality' column is: {max_quality}")
print(f"The lowest value in the 'quality' column is: {min_quality}")

"""How is `fixed acidity` correlated to the quality of the wine? How does the alcohol content affect the quality? How is the `free Sulphur dioxide` content correlated to the quality of the wine?"""

attribute1 = df['fixed acidity']
attribute2 = df['quality']
correlation = attribute1.corr(attribute2)
print(f'Correlation between Fixed acidity and Quality of wine: {correlation}')

attribute1 = df['alcohol']
attribute2 = df['quality']
correlation = attribute1.corr(attribute2)
print(f'Correlation between Alcohol and Quality of wine: {correlation}')

attribute1 = df['free sulfur dioxide']
attribute2 = df['quality']
correlation = attribute1.corr(attribute2)
print(f'Correlation between Free sulphur dioxide and Quality of wine: {correlation}')

"""What is the average `residual sugar` for the best quality wine and the lowest quality wine in the dataset?"""

best_quality = df['quality'].max()
lowest_quality = df['quality'].min()

average_residual_sugar_best = df[df['quality'] == best_quality]['residual sugar'].mean()
print(f"The average residual sugar for the best quality wine (quality = {best_quality}) is: {average_residual_sugar_best}")
average_residual_sugar_lowest = df[df['quality'] == lowest_quality]['residual sugar'].mean()
print(f"The average residual sugar for the lowest quality wine (quality = {lowest_quality}) is: {average_residual_sugar_lowest}")

"""Does `volatile acidity` has an effect over the quality of the wine samples in the dataset?"""

attribute1 = df['volatile acidity']
attribute2 = df['quality']
correlation = attribute1.corr(attribute2)
print(f'Correlation between Volatile acidity and Quality of wine: {correlation}')

"""Since it has a negative correlation volatile acidity in low numbers would mean better quality of the wine.
The correlation constant is quite low therefore it would also mean that it doesn't have a very dominating impact on the quality of wine.

Training the models
"""

df = df.dropna()

df = df.drop_duplicates()

x = df.iloc[:, :-1].values
y = df.iloc[:, -1].values

x_train, x_test, y_train, y_test = train_test_split(x, y, train_size= 0.75, shuffle=True, random_state = 1)

sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

from sklearn.tree import DecisionTreeClassifier
classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier.fit(x_train,y_train)

y_pred = classifier.predict(x_test)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
print(cm)
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_pred))

from sklearn.ensemble import RandomForestClassifier
classifier1 = RandomForestClassifier(n_estimators=17, criterion='entropy', random_state=0)
classifier1.fit(x_train, y_train)

y_pred = classifier1.predict(x_test)

cm = confusion_matrix(y_test, y_pred)
print(cm)
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_pred))

"""Making the n estimators 17 I was able to increase the model's efficiency by 7 percent. Therefore the Random Forest Classifier is a more efficient model compared to the decision tree."""

